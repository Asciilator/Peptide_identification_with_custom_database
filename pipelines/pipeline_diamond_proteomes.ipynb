{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d21523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path of the project: c:\\Users\\Yusuf\\OneDrive\\LST\\Derde_jaar\\Y3Q4\\Metaproteomics_with_db\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Automatically get the base path of your project\n",
    "base_path = Path.cwd().parents[0]  # adjust .parents[0] if needed\n",
    "print(\"Base path of the project:\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is more robust and will work regardless of the current working directory.\n",
    "# Make sure that base_path is defined correctly.\n",
    "# Ensure the required packages are installed from the requirements.txt file\n",
    "#!pip3 install -r \"{base_path}/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2035cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the path to your CSV file between the quotes below\n",
    "input_path = r\"C:\\Users\\Yusuf\\OneDrive\\LST\\Derde_jaar\\Y3Q4\\Metaproteomics_with_db\\pipelines\\Diamond_alignments\\diamond_df_lca_filtered_5_hits_by_genus.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b6ffa",
   "metadata": {},
   "source": [
    "### Step 13: Fetch Proteomes for Organisms Using a Ranked Strategy\n",
    "\n",
    "This cell initiates the *proteome-based strategy* to collect protein data for custom database creation. Instead of directly retrieving proteins from UniProtKB, this approach:\n",
    "- Queries **whole proteomes** from UniProt based on the identified organisms.\n",
    "- **Weights proteome downloads** by the number of peptide hits per organism (top 5 get more proteomes, lower-ranking ones get fewer).\n",
    "- Ensures that high-confidence organisms are well-represented while maintaining a manageable dataset size.\n",
    "\n",
    "This strategy allows the construction of a biologically realistic and computationally efficient protein database enriched with full proteomes of organisms most likely present in the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247a808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching proteomes based on rank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:13<00:00,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "Proteomes matched for 91 / 116 organisms\n",
      "No proteomes found for: 25\n",
      "Output saved to: Diamond_alignments/session_diamond_align_2025-06-08_00-28-56\\diamond_proteome_matches_all.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# === 1. Load Diamond alignment results and setup  output path ===\n",
    "\n",
    "# Load previously annotated file (contains organism names and taxonomic ranks)\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Create a timestamped output folder for this DIAMOND session\n",
    "session_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Diamond_alignments/session_diamond_align_{session_time}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define path for output CSV file containing matched proteomes\n",
    "proteome_csv_path = os.path.join(output_dir, \"diamond_proteome_matches_all.csv\")\n",
    "\n",
    "# === 2. Proteome limits per organism ranking group ===\n",
    "\n",
    "# More peptide hits = fetch more proteomes\n",
    "limits = {\n",
    "    \"top_5\": 100,         # Organisms ranked 1–5\n",
    "    \"rank_6_10\": 50,      # Organisms ranked 6–10\n",
    "    \"rank_11_20\": 30,     # Organisms ranked 11–20\n",
    "    \"rank_21_50\": 10,      # Organisms ranked 21–50\n",
    "    \"rank_51_plus\": 1     # All others\n",
    "}\n",
    "\n",
    "# === 3. Count peptide hits ===\n",
    "\n",
    "# Count how often each organism appears (i.e., number of peptide hits)\n",
    "organism_counts = df[\"lca_taxon_name\"].value_counts()\n",
    "\n",
    "# Create an ordered list of organisms based on abundance\n",
    "ranked_organisms = organism_counts.index.tolist()\n",
    "\n",
    "# Create a lookup: organism name → its assigned taxonomy rank\n",
    "rank_lookup = df.drop_duplicates(\"lca_taxon_name\").set_index(\"lca_taxon_name\")[\"lca_rank\"].to_dict()\n",
    "\n",
    "# === 4. Plan how many proteomes to fetch per organism ===\n",
    "\n",
    "fetch_plan = []\n",
    "for i, org in enumerate(ranked_organisms):\n",
    "    if i < 5:\n",
    "        n = limits[\"top_5\"]\n",
    "    elif i < 10:\n",
    "        n = limits[\"rank_6_10\"]\n",
    "    elif i < 20:\n",
    "        n = limits[\"rank_11_20\"]\n",
    "    elif i < 50:\n",
    "        n = limits[\"rank_21_50\"]\n",
    "    else:\n",
    "        n = limits[\"rank_51_plus\"]\n",
    "    \n",
    "    # Add to fetch plan: (ranking index, organism name, taxonomic rank, proteomes to fetch)\n",
    "    fetch_plan.append((i + 1, org, rank_lookup.get(org, \"unknown\"), n))\n",
    "\n",
    "# === 5. Function to fetch top proteomes for a given organism ===\n",
    "\n",
    "def fetch_proteomes(organism, rank, n):\n",
    "    \"\"\"\n",
    "    Queries UniProt's REST API to fetch up to `n` proteomes for a given organism.\n",
    "    Prioritizes 'Reference' proteomes, then sorts by BUSCO completeness.\n",
    "    \"\"\"\n",
    "    base_url = \"https://rest.uniprot.org/proteomes/search\"\n",
    "    \n",
    "    try:\n",
    "        # Query UniProt API with organism name\n",
    "        r = requests.get(base_url, params={\"query\": organism, \"format\": \"json\", \"size\": 500})\n",
    "        r.raise_for_status()\n",
    "        results = r.json().get(\"results\", [])\n",
    "    except Exception:\n",
    "        return []  # On error or no results, return empty list\n",
    "\n",
    "    # Prioritize reference proteomes first\n",
    "    reference = [r for r in results if r.get(\"proteomeType\") == \"Reference\"]\n",
    "    if reference:\n",
    "        results = reference + [r for r in results if r not in reference]\n",
    "\n",
    "    # Sort by completeness using BUSCO scores\n",
    "    results = sorted(results, key=lambda r: r.get(\"busco\", {}).get(\"complete\", 0), reverse=True)\n",
    "\n",
    "    # Return top `n` proteomes (structured)\n",
    "    return [\n",
    "        (\n",
    "            organism,\n",
    "            r.get(\"id\"),\n",
    "            r.get(\"proteomeType\"),\n",
    "            r.get(\"taxonomy\", {}).get(\"rank\", \"unknown\"),\n",
    "            \"name\"  # Placeholder; you can optionally include r.get(\"name\")\n",
    "        )\n",
    "        for r in results[:n]\n",
    "    ]\n",
    "\n",
    "# === 6. Run fetch plan and collect results ===\n",
    "\n",
    "results = []      # Holds all proteomes fetched\n",
    "not_found = []    # Track organisms for which no proteomes were found\n",
    "\n",
    "print(\"\\nFetching proteomes based on rank...\")\n",
    "\n",
    "# Iterate through fetch plan and retrieve proteomes\n",
    "for rank_idx, org, taxrank, limit in tqdm(fetch_plan):\n",
    "    hits = fetch_proteomes(org, taxrank, limit)\n",
    "    if hits:\n",
    "        results.extend(hits)\n",
    "    else:\n",
    "        not_found.append(org)\n",
    "\n",
    "# === 7. Save matched proteomes to CSV ===\n",
    "\n",
    "df_out = pd.DataFrame(results, columns=[\"Organism\", \"Proteome ID\", \"Proteome Type\", \"Tax Rank\", \"Used Query\"])\n",
    "df_out.to_csv(proteome_csv_path, index=False)\n",
    "\n",
    "# === 8. Summary logging ===\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Proteomes matched for {len(set([r[0] for r in results]))} / {len(ranked_organisms)} organisms\")\n",
    "print(f\"No proteomes found for: {len(not_found)}\")\n",
    "print(f\"Output saved to: {proteome_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36887fe",
   "metadata": {},
   "source": [
    "### Step 14: Download and Merge Protein FASTA Files from Matched Proteomes\n",
    "\n",
    "After selecting organisms based on DIAMOND peptide hits and fetching their matching UniProt proteomes, this cell automates the download of **protein sequences** from each matched proteome.\n",
    "\n",
    "Key features:\n",
    "- Uses UniProt’s `/uniprotkb/stream` API to fetch FASTA data.\n",
    "- Parallelizes requests using `ThreadPoolExecutor` to accelerate download.\n",
    "- Collects and merges all results into a single multi-entry FASTA file.\n",
    "- Skips failed downloads and prints a list of any failed proteomes.\n",
    "\n",
    "This final merged FASTA file will be used to construct a custom database for downstream taxonomic or functional annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb80b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading 465 proteomes using 12 threads...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 465/465 [08:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.\n",
      "Total protein sequences written: 982281\n",
      "Output FASTA saved to:\n",
      "Diamond_alignments/session_diamond_align_2025-06-08_00-28-56\\diamond_proteins_from_proteomes.fasta\n",
      "151 proteomes failed to download.\n",
      "Failed Proteome IDs: ['UP001331748', 'UP000638128', 'UP001437458', 'UP000749010', 'UP001205616', 'UP001475101', 'UP000399556', 'UP001334079', 'UP001337489', 'UP001306933', 'UP000595825', 'UP000642269', 'UP001334098', 'UP000655783', 'UP001310659', 'UP000753056', 'UP001341805', 'UP001270871', 'UP001337352', 'UP001290869', 'UP001310474', 'UP001311020', 'UP001311888', 'UP001326372', 'UP001337599', 'UP001342094', 'UP001344291', 'UP000192803', 'UP000192543', 'UP001335471', 'UP000242305', 'UP000243131', 'UP000250418', 'UP001337090', 'UP001347060', 'UP001353517', 'UP000192299', 'UP000192438', 'UP000192584', 'UP000192681', 'UP000243903', 'UP000243945', 'UP000250530', 'UP000630262', 'UP001194621', 'UP000595364', 'UP000747980', 'UP001194612', 'UP001195010', 'UP001208091', 'UP001343535', 'UP001280549', 'UP001323820', 'UP001303235', 'UP001306786', 'UP001350276', 'UP001306892', 'UP001313845', 'UP001337416', 'UP001342545', 'UP001344827', 'UP001348359', 'UP001350556', 'UP001352257', 'UP000620421', 'UP000578785', 'UP000777475', 'UP000629904', 'UP000657165', 'UP000672947', 'UP000673570', 'UP001055019', 'UP001055041', 'UP001055196', 'UP001277902', 'UP001148366', 'UP001181209', 'UP001238914', 'UP001148339', 'UP001181137', 'UP001181184', 'UP001306169', 'UP001201616', 'UP001209901', 'UP001223667', 'UP001302114', 'UP001276260', 'UP001277913', 'UP001312341', 'UP001312117', 'UP001307779', 'UP001281123', 'UP001283638', 'UP001288488', 'UP001289428', 'UP001307639', 'UP001309236', 'UP001309641', 'UP001311050', 'UP001312009', 'UP001312564', 'UP001313367', 'UP001313973', 'UP001195460', 'UP001324324', 'UP001325110', 'UP001326306', 'UP000465081', 'UP000595957', 'UP000054945', 'UP000198622', 'UP000070208', 'UP000485549', 'UP000276503', 'UP000176671', 'UP001330572', 'UP001483331', 'UP001391626', 'UP001441552', 'UP001445934', 'UP001445429', 'UP000648285', 'UP001449924', 'UP001397147', 'UP001451919', 'UP001434805', 'UP000672380', 'UP000673666', 'UP000674560', 'UP000198861', 'UP001187344', 'UP001340899', 'UP001341714', 'UP001344078', 'UP001356891', 'UP001264329', 'UP001195110', 'UP000198254', 'UP001374378', 'UP000573270', 'UP001374288', 'UP000675880', 'UP000178963', 'UP000316519', 'UP000315633', 'UP001514861', 'UP001514862', 'UP001514860', 'UP000707887', 'UP000176467', 'UP001198013']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# === 1. Config ===\n",
    "\n",
    "# Directory where previous proteome matching results were saved\n",
    "output_dir = os.path.dirname(proteome_csv_path)\n",
    "\n",
    "# Output path for the combined FASTA containing all downloaded proteins\n",
    "output_fasta = os.path.join(output_dir, \"diamond_proteins_from_proteomes.fasta\")\n",
    "\n",
    "# Determine a reasonable number of threads to use (does not overload CPU because downloads are I/O-bound)\n",
    "num_threads = os.cpu_count()\n",
    "\n",
    "# === 2. Load proteome IDs ===\n",
    "\n",
    "# Read CSV file created from matched proteomes\n",
    "df = pd.read_csv(proteome_csv_path)\n",
    "\n",
    "# Extract unique proteome IDs (to avoid redundant downloads)\n",
    "proteome_ids = df[\"Proteome ID\"].dropna().unique()\n",
    "\n",
    "# === 3. Define the FASTA download function ===\n",
    "\n",
    "def fetch_fasta(proteome_id):\n",
    "    \"\"\"\n",
    "    Fetches all protein sequences in FASTA format from a given UniProt proteome ID.\n",
    "    Returns (proteome_id, fasta_content) on success, (proteome_id, None) on failure.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/stream?query=proteome:{proteome_id}&format=fasta\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=60)\n",
    "        if response.ok:\n",
    "            return proteome_id, response.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    return proteome_id, None\n",
    "\n",
    "# === 4. Parallel downloading of proteomes ===\n",
    "\n",
    "all_entries = []      # To store all downloaded protein FASTA entries\n",
    "failed_ids = []       # To track proteomes that could not be downloaded\n",
    "\n",
    "print(f\"\\nDownloading {len(proteome_ids)} proteomes using {num_threads} threads...\\n\")\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize requests for better performance\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    # Submit all proteome download tasks at once\n",
    "    futures = {executor.submit(fetch_fasta, pid): pid for pid in proteome_ids}\n",
    "\n",
    "    # Process results as they complete\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        pid, result = future.result()\n",
    "        if result:\n",
    "            # Split result into FASTA entries (entries start with \">\")\n",
    "            entries = result.split(\"\\n>\")\n",
    "            for entry in entries:\n",
    "                # Ensure proper formatting (each entry must start with \">\")\n",
    "                if not entry.startswith(\">\"):\n",
    "                    entry = \">\" + entry\n",
    "                all_entries.append(entry)\n",
    "        else:\n",
    "            failed_ids.append(pid)\n",
    "\n",
    "# === 5. Write all protein sequences to one combined FASTA file ===\n",
    "\n",
    "with open(output_fasta, \"w\") as f_out:\n",
    "    f_out.write(\"\\n\".join(all_entries))\n",
    "\n",
    "# === 6. Summary output ===\n",
    "\n",
    "print(\"\\nDownload complete.\")\n",
    "print(f\"Total protein sequences written: {len(all_entries)}\")\n",
    "print(f\"Output FASTA saved to:\\n{output_fasta}\")\n",
    "\n",
    "if failed_ids:\n",
    "    print(f\"{len(failed_ids)} proteomes failed to download.\")\n",
    "    print(\"Failed Proteome IDs:\", failed_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
