{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d21523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path of the project: c:\\Users\\Yusuf\\OneDrive\\LST\\Derde_jaar\\Y3Q4\\Metaproteomics_with_db\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Automatically get the base path of your project\n",
    "base_path = Path.cwd().parents[0]  # adjust .parents[0] if needed\n",
    "print(\"Base path of the project:\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is more robust and will work regardless of the current working directory.\n",
    "# Make sure that base_path is defined correctly.\n",
    "# Ensure the required packages are installed from the requirements.txt file\n",
    "#!pip3 install -r \"{base_path}/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b6ffa",
   "metadata": {},
   "source": [
    "### Step 13: Fetch Proteomes for Organisms Using a Ranked Strategy\n",
    "\n",
    "This cell initiates the *proteome-based strategy* to collect protein data for custom database creation. Instead of directly retrieving proteins from UniProtKB, this approach:\n",
    "- Queries **whole proteomes** from UniProt based on the identified organisms.\n",
    "- **Weights proteome downloads** by the number of peptide hits per organism (top 5 get more proteomes, lower-ranking ones get fewer).\n",
    "- Ensures that high-confidence organisms are well-represented while maintaining a manageable dataset size.\n",
    "\n",
    "This strategy allows the construction of a biologically realistic and computationally efficient protein database enriched with full proteomes of organisms most likely present in the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247a808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Fetching proteomes based on rank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:29<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š === SUMMARY ===\n",
      "âœ… Proteomes matched for 261 / 278 organisms\n",
      "âŒ No proteomes found for: 17\n",
      "ðŸ“ Output saved to: Diamond_alignments/session_diamond_align_2025-06-02_17-23-44\\diamond_proteome_matches_all.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# === 1. Setup output path ===\n",
    "\n",
    "# Create a timestamped output folder for this DIAMOND session\n",
    "session_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"Diamond_alignments/session_diamond_align_{session_time}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define path for output CSV file containing matched proteomes\n",
    "proteome_csv_path = os.path.join(output_dir, \"diamond_proteome_matches_all.csv\")\n",
    "\n",
    "# === 2. Proteome limits per organism ranking group ===\n",
    "\n",
    "# More peptide hits = fetch more proteomes\n",
    "limits = {\n",
    "    \"top_5\": 100,         # Organisms ranked 1â€“5\n",
    "    \"rank_6_10\": 50,      # Organisms ranked 6â€“10\n",
    "    \"rank_11_20\": 20,     # Organisms ranked 11â€“20\n",
    "    \"rank_21_50\": 5,      # Organisms ranked 21â€“50\n",
    "    \"rank_51_plus\": 1     # All others\n",
    "}\n",
    "\n",
    "# === 3. Load DIAMOND alignment results + count peptide hits ===\n",
    "\n",
    "# Load previously annotated file (contains organism names and taxonomic ranks)\n",
    "df = pd.read_csv(\"Diamond_alignments/diamond_df_rank_annotated_fixed.csv\")\n",
    "\n",
    "# Count how often each organism appears (i.e., number of peptide hits)\n",
    "organism_counts = df[\"organism\"].value_counts()\n",
    "\n",
    "# Create an ordered list of organisms based on abundance\n",
    "ranked_organisms = organism_counts.index.tolist()\n",
    "\n",
    "# Create a lookup: organism name â†’ its assigned taxonomy rank\n",
    "rank_lookup = df.drop_duplicates(\"organism\").set_index(\"organism\")[\"taxonomy_rank\"].to_dict()\n",
    "\n",
    "# === 4. Plan how many proteomes to fetch per organism ===\n",
    "\n",
    "fetch_plan = []\n",
    "for i, org in enumerate(ranked_organisms):\n",
    "    if i < 5:\n",
    "        n = limits[\"top_5\"]\n",
    "    elif i < 10:\n",
    "        n = limits[\"rank_6_10\"]\n",
    "    elif i < 20:\n",
    "        n = limits[\"rank_11_20\"]\n",
    "    elif i < 50:\n",
    "        n = limits[\"rank_21_50\"]\n",
    "    else:\n",
    "        n = limits[\"rank_51_plus\"]\n",
    "    \n",
    "    # Add to fetch plan: (ranking index, organism name, taxonomic rank, proteomes to fetch)\n",
    "    fetch_plan.append((i + 1, org, rank_lookup.get(org, \"unknown\"), n))\n",
    "\n",
    "# === 5. Function to fetch top proteomes for a given organism ===\n",
    "\n",
    "def fetch_proteomes(organism, rank, n):\n",
    "    \"\"\"\n",
    "    Queries UniProt's REST API to fetch up to `n` proteomes for a given organism.\n",
    "    Prioritizes 'Reference' proteomes, then sorts by BUSCO completeness.\n",
    "    \"\"\"\n",
    "    base_url = \"https://rest.uniprot.org/proteomes/search\"\n",
    "    \n",
    "    try:\n",
    "        # Query UniProt API with organism name\n",
    "        r = requests.get(base_url, params={\"query\": organism, \"format\": \"json\", \"size\": 500})\n",
    "        r.raise_for_status()\n",
    "        results = r.json().get(\"results\", [])\n",
    "    except Exception:\n",
    "        return []  # On error or no results, return empty list\n",
    "\n",
    "    # Prioritize reference proteomes first\n",
    "    reference = [r for r in results if r.get(\"proteomeType\") == \"Reference\"]\n",
    "    if reference:\n",
    "        results = reference + [r for r in results if r not in reference]\n",
    "\n",
    "    # Sort by completeness using BUSCO scores\n",
    "    results = sorted(results, key=lambda r: r.get(\"busco\", {}).get(\"complete\", 0), reverse=True)\n",
    "\n",
    "    # Return top `n` proteomes (structured)\n",
    "    return [\n",
    "        (\n",
    "            organism,\n",
    "            r.get(\"id\"),\n",
    "            r.get(\"proteomeType\"),\n",
    "            r.get(\"taxonomy\", {}).get(\"rank\", \"unknown\"),\n",
    "            \"name\"  # Placeholder; you can optionally include r.get(\"name\")\n",
    "        )\n",
    "        for r in results[:n]\n",
    "    ]\n",
    "\n",
    "# === 6. Run fetch plan and collect results ===\n",
    "\n",
    "results = []      # Holds all proteomes fetched\n",
    "not_found = []    # Track organisms for which no proteomes were found\n",
    "\n",
    "print(\"\\nFetching proteomes based on rank...\")\n",
    "\n",
    "# Iterate through fetch plan and retrieve proteomes\n",
    "for rank_idx, org, taxrank, limit in tqdm(fetch_plan):\n",
    "    hits = fetch_proteomes(org, taxrank, limit)\n",
    "    if hits:\n",
    "        results.extend(hits)\n",
    "    else:\n",
    "        not_found.append(org)\n",
    "\n",
    "# === 7. Save matched proteomes to CSV ===\n",
    "\n",
    "df_out = pd.DataFrame(results, columns=[\"Organism\", \"Proteome ID\", \"Proteome Type\", \"Tax Rank\", \"Used Query\"])\n",
    "df_out.to_csv(proteome_csv_path, index=False)\n",
    "\n",
    "# === 8. Summary logging ===\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Proteomes matched for {len(set([r[0] for r in results]))} / {len(ranked_organisms)} organisms\")\n",
    "print(f\"No proteomes found for: {len(not_found)}\")\n",
    "print(f\"Output saved to: {proteome_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36887fe",
   "metadata": {},
   "source": [
    "### Step 14: Download and Merge Protein FASTA Files from Matched Proteomes\n",
    "\n",
    "After selecting organisms based on DIAMOND peptide hits and fetching their matching UniProt proteomes, this cell automates the download of **protein sequences** from each matched proteome.\n",
    "\n",
    "Key features:\n",
    "- Uses UniProtâ€™s `/uniprotkb/stream` API to fetch FASTA data.\n",
    "- Parallelizes requests using `ThreadPoolExecutor` to accelerate download.\n",
    "- Collects and merges all results into a single multi-entry FASTA file.\n",
    "- Skips failed downloads and prints a list of any failed proteomes.\n",
    "\n",
    "This final merged FASTA file will be used to construct a custom database for downstream taxonomic or functional annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Downloading FASTA files for 923 proteomes using 8 threads...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 923/923 [24:11<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Download complete.\n",
      "âœ… Total sequences written: 2755606\n",
      "âŒ Proteomes failed to download: 196\n",
      "Failed IDs: ['UP000257281', 'UP000260238', 'UP000606398', 'UP000594619', 'UP000618573', 'UP000608564', 'UP000640770', 'UP000641310', 'UP000672553', 'UP000636347', 'UP000619799', 'UP000672815', 'UP000673606', 'UP000623454', 'UP000673624', 'UP000674615', 'UP000636420', 'UP000675066', 'UP000651759', 'UP000672736', 'UP000673195', 'UP000673466', 'UP000673536', 'UP000717363', 'UP000767385', 'UP000778227', 'UP000807157', 'UP001057786', 'UP000674281', 'UP001057842', 'UP000709408', 'UP001057854', 'UP000710986', 'UP001057884', 'UP000770306', 'UP000772915', 'UP000811667', 'UP000826969', 'UP001057836', 'UP001159746', 'UP001057852', 'UP001059466', 'UP001059564', 'UP001159151', 'UP001159715', 'UP001159720', 'UP001159738', 'UP001159756', 'UP001159771', 'UP001159801', 'UP001161004', 'UP000316179', 'UP000601385', 'UP000596954', 'UP000602056', 'UP000620433', 'UP000631491', 'UP000643674', 'UP000663967', 'UP000666831', 'UP000651019', 'UP000668294', 'UP000664103', 'UP001181337', 'UP000674899', 'UP000664662', 'UP000663973', 'UP001245605', 'UP000664439', 'UP001246342', 'UP000670651', 'UP000673188', 'UP001251814', 'UP000673239', 'UP001251904', 'UP001254404', 'UP001255852', 'UP001195330', 'UP001259402', 'UP001205377', 'UP001206610', 'UP001262312', 'UP001249628', 'UP001266165', 'UP001250956', 'UP001306478', 'UP001306774', 'UP001309764', 'UP001314054', 'UP001251221', 'UP001253259', 'UP001256889', 'UP001322049', 'UP001322130', 'UP001322080', 'UP001261392', 'UP001322093', 'UP001322288', 'UP001261508', 'UP001322799', 'UP001266220', 'UP001322381', 'UP001268800', 'UP001321885', 'UP001322416', 'UP001323116', 'UP001322344', 'UP001323423', 'UP001323140', 'UP001322630', 'UP001324140', 'UP001324390', 'UP001325036', 'UP001325825', 'UP001326070', 'UP001323029', 'UP001323066', 'UP001326644', 'UP001326185', 'UP001323145', 'UP001326701', 'UP001323482', 'UP001323551', 'UP001327130', 'UP001325011', 'UP001326076', 'UP001326465', 'UP001333177', 'UP001326663', 'UP001341664', 'UP001341793', 'UP001326723', 'UP001326776', 'UP001326969', 'UP001327305', 'UP001328975', 'UP001331483', 'UP001336069', 'UP001343090', 'UP000595825', 'UP000642269', 'UP000655783', 'UP000753056', 'UP001306933', 'UP001270871', 'UP001290869', 'UP001311888', 'UP001311020', 'UP001326372', 'UP001335471', 'UP001337599', 'UP001310474', 'UP001342094', 'UP001310659', 'UP001334098', 'UP001337090', 'UP001353517', 'UP001337352', 'UP001341805', 'UP001347060', 'UP001344291', 'UP000638128', 'UP000399556', 'UP000749010', 'UP001337489', 'UP001205616', 'UP001331748', 'UP001334079', 'UP001437458', 'UP001475101', 'UP000275248', 'UP000262069', 'UP000321076', 'UP000617755', 'UP000623065', 'UP000602525', 'UP000614951', 'UP000647026', 'UP000653972', 'UP000664320', 'UP000198622', 'UP001195110', 'UP000675880', 'UP000617715', 'UP000673507', 'UP000673548', 'UP001055169', 'UP000672201', 'UP000672817', 'UP000673222', 'UP000707887', 'UP001198013', 'UP001198179', 'UP001198207', 'UP001197729', 'UP001302684']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# === 1. Config ===\n",
    "\n",
    "# Directory where previous proteome matching results were saved\n",
    "output_dir = os.path.dirname(proteome_csv_path)\n",
    "\n",
    "# Output path for the combined FASTA containing all downloaded proteins\n",
    "output_fasta = os.path.join(output_dir, \"diamond_proteins_from_proteomes.fasta\")\n",
    "\n",
    "# Determine a reasonable number of threads to use (does not overload CPU because downloads are I/O-bound)\n",
    "num_threads = os.cpu_count()\n",
    "\n",
    "# === 2. Load proteome IDs ===\n",
    "\n",
    "# Read CSV file created from matched proteomes\n",
    "df = pd.read_csv(proteome_csv_path)\n",
    "\n",
    "# Extract unique proteome IDs (to avoid redundant downloads)\n",
    "proteome_ids = df[\"Proteome ID\"].dropna().unique()\n",
    "\n",
    "# === 3. Define the FASTA download function ===\n",
    "\n",
    "def fetch_fasta(proteome_id):\n",
    "    \"\"\"\n",
    "    Fetches all protein sequences in FASTA format from a given UniProt proteome ID.\n",
    "    Returns (proteome_id, fasta_content) on success, (proteome_id, None) on failure.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/stream?query=proteome:{proteome_id}&format=fasta\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=60)\n",
    "        if response.ok:\n",
    "            return proteome_id, response.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    return proteome_id, None\n",
    "\n",
    "# === 4. Parallel downloading of proteomes ===\n",
    "\n",
    "all_entries = []      # To store all downloaded protein FASTA entries\n",
    "failed_ids = []       # To track proteomes that could not be downloaded\n",
    "\n",
    "print(f\"\\nDownloading {len(proteome_ids)} proteomes using {num_threads} threads...\\n\")\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize requests for better performance\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    # Submit all proteome download tasks at once\n",
    "    futures = {executor.submit(fetch_fasta, pid): pid for pid in proteome_ids}\n",
    "\n",
    "    # Process results as they complete\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        pid, result = future.result()\n",
    "        if result:\n",
    "            # Split result into FASTA entries (entries start with \">\")\n",
    "            entries = result.split(\"\\n>\")\n",
    "            for entry in entries:\n",
    "                # Ensure proper formatting (each entry must start with \">\")\n",
    "                if not entry.startswith(\">\"):\n",
    "                    entry = \">\" + entry\n",
    "                all_entries.append(entry)\n",
    "        else:\n",
    "            failed_ids.append(pid)\n",
    "\n",
    "# === 5. Write all protein sequences to one combined FASTA file ===\n",
    "\n",
    "with open(output_fasta, \"w\") as f_out:\n",
    "    f_out.write(\"\\n\".join(all_entries))\n",
    "\n",
    "# === 6. Summary output ===\n",
    "\n",
    "print(\"\\nDownload complete.\")\n",
    "print(f\"Total protein sequences written: {len(all_entries)}\")\n",
    "print(f\"Output FASTA saved to:\\n{output_fasta}\")\n",
    "\n",
    "if failed_ids:\n",
    "    print(f\"{len(failed_ids)} proteomes failed to download.\")\n",
    "    print(\"Failed Proteome IDs:\", failed_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
